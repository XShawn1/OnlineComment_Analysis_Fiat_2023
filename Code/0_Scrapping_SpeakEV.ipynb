{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Simulate the Interface of SpeakEV with Selenium\n",
    "### 1-1 Set custom headers to pretend as a visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare the headers to pretend as a real visitor\n",
    "# set the user agent\n",
    "headers = {\"accept\":\"*/*\", \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 Simulate the interface and enter the main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### simulate SpeakEV's webpage\n",
    "opts = Options()\n",
    "opts.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.get(\"https://www.speakev.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "# click the cookies button\n",
    "cookie_button = driver.find_element(By.XPATH,'//*[@class=\"sc-ifAKCX ljEJIv\"]')\n",
    "cookie_button.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Record URLs Linked to Posts\n",
    "### Two Approaches: \n",
    "### 1st - via the specific forum navigator \n",
    "### 2nd - via the insite keyword searching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Approach 1: Find the forums under the targeted topic through the navigator\n",
    "### Jump to the navigator first and record the urls of related forumns that includes the keyword (brand name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Identify the navigator and move to the result page\n",
    "search_button = driver.find_element(By.XPATH,'//a[@class=\"new-navigation vl-middle wayfinding-2  button\"]')\n",
    "search_button.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiat EV Forum\n",
      "2 forums found\n",
      "['https://www.speakev.com/forums/general-fiat-discussion.360/', 'https://www.speakev.com/forums/fiat-500.361/']\n"
     ]
    }
   ],
   "source": [
    "#### Record all topic links to the forums\n",
    "l_topic = driver.find_elements(By.XPATH,'//h3[@class=\"node-title\"]')\n",
    "# find the links that include keywords\n",
    "l_topic_link = []\n",
    "l_topic_link = [i for i in l_topic if \"Fiat\" in i.find_element_by_css_selector(\"a\").text] \n",
    "for i in l_topic_link:\n",
    "    print(i.find_element_by_css_selector(\"a\").text)\n",
    "time.sleep(3)\n",
    "\n",
    "# build up a list of urls of each post under the topics\n",
    "forums_urls = []\n",
    "\n",
    "#### move to the topic page that contains forums with the keywords\n",
    "for topic in l_topic_link:\n",
    "    if topic.is_enabled():\n",
    "        # the topics are sometimes intercepted to be clicked, it's not sure if the ip is blocked or because the webpage structure is dynamic.\n",
    "        # As the topics are blocked, we choose to jump to the next page via urls directly\n",
    "        url_topic = topic.find_element_by_css_selector(\"a\").get_attribute('href')\n",
    "        driver.get(url_topic)\n",
    "        time.sleep(3)\n",
    "        #### Store the urls of each forum under the topic\n",
    "        stop = False\n",
    "        \n",
    "        # use an embedded loop to collect urls\n",
    "        while not stop:\n",
    "            # identify and store urls into the list\n",
    "            forums = driver.find_elements(By.XPATH,'//*[@class=\"node-title\"]')\n",
    "            print(str(len(forums)) + \" forums found\")\n",
    "            for forum in forums:\n",
    "                forum_url = forum.find_element_by_css_selector(\"a\")\n",
    "                forums_urls.append(forum_url.get_attribute('href'))\n",
    "            next_arrows = driver.find_elements(By.XPATH,'//a[@class=\"pageNav-jump pageNav-jump--next california-page-nav-jump-next\"]')\n",
    "            if len(next_arrows) > 0:\n",
    "                next_arrow = next_arrows[0]\n",
    "                if next_arrow.is_enabled():\n",
    "                    next_arrow.click()\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    stop = True\n",
    "            else:\n",
    "                stop = True\n",
    "        print(forums_urls[0:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Approach 2: Use the keyword to find the posts that have relevant titles\n",
    "### Jump to the search page and specify the search settings, then jump to the result page where posts have titles that include the keyword (brand name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use the search page to navigate to the posts relevant to the keyword\n",
    "# go to the search page\n",
    "search_url = \"https://www.speakev.com/search/?type=post\"\n",
    "driver.get(search_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# input the keyword into the search box\n",
    "search_box = driver.find_element(By.XPATH,'//*[@class=\"input search-form__field\"]')\n",
    "search_box.send_keys(\"Fiat\")\n",
    "\n",
    "# select the \"title only\" checkbox to target posts whose title contains the keyword\n",
    "titleonly_box = driver.find_element(By.XPATH,'//*[@qid=\"title-only-checkbox\"]')\n",
    "if not titleonly_box.is_selected():\n",
    "    driver.execute_script(\"arguments[0].click();\", titleonly_box)\n",
    "\n",
    "# unselect the \"subforum\" checkbox to exclude sub forums\n",
    "subforum_box = driver.find_element(By.XPATH,'//*[@qid=\"include-subforums-check-box\"]')\n",
    "if subforum_box.is_selected():\n",
    "    driver.execute_script(\"arguments[0].click();\", subforum_box)\n",
    "\n",
    "# click the \"search\" button to jump to the result page\n",
    "search_button = driver.find_element(By.XPATH,'//*[@qid=\"search-button\"]')\n",
    "driver.execute_script(\"arguments[0].click();\", search_button)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Find posts within each forum and record the urls\n",
    "### This phase is logically connected to phase 2, and the two approaches aren't expected to be executed all together. Either approach is expected to be excecuted independently within only one simulated interface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Tesla, there are too many forums and comments, so only the most popular general forum was scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up a list of urls of each post\n",
    "posting_urls = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 postings found\n",
      "35 postings found\n",
      "35 postings found\n",
      "6 postings found\n",
      "81 ['https://www.speakev.com/threads/fiat-doesnt-like-transparency.176618/', 'https://www.speakev.com/threads/fiat-600e-500x-replacement-to-be-announced-in-a-few-weeks.177628/', 'https://www.speakev.com/threads/new-fiat-panda-coming-most-affordable-ev.165895/', 'https://www.speakev.com/threads/fiat-all-electrified.169789/', 'https://www.speakev.com/threads/should-this-forum-move-to-be-part-of-a-stellantis-group-section.164981/']\n"
     ]
    }
   ],
   "source": [
    "#### Store the urls of each post within each forum (When following Approach 1)\n",
    "# Manually set the forum for Tesla\n",
    "# forums_urls = [\"\"]\n",
    "\n",
    "for forum in forums_urls:\n",
    "    # enter each forum:\n",
    "    driver.get(forum)\n",
    "    time.sleep(5)\n",
    "    # identify and store urls into the list\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        postings = driver.find_elements(By.XPATH,'//*[@class=\"structItem-title\"]')\n",
    "        print(str(len(postings)) + \" postings found\")\n",
    "        for posting in postings:\n",
    "            posting_url = posting.find_element_by_css_selector(\"a\")\n",
    "            posting_urls.append(posting_url.get_attribute('href'))\n",
    "        # use try-except to avoid cases where the there is no \"next page\" button\n",
    "        try:\n",
    "            next_arrows = driver.find_elements(By.XPATH,'//a[@class=\"pageNavSimple-el pageNavSimple-el--next\"]')\n",
    "            if len(next_arrows)>0:\n",
    "                next_arrow = next_arrows[0]\n",
    "                if next_arrow.is_enabled():\n",
    "                    # button is not interactable here, jump to the next page via the url directly\n",
    "                    url_next_page = next_arrow.get_attribute('href')\n",
    "                    driver.get(url_next_page)\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    stop = True\n",
    "            else:\n",
    "                stop = True\n",
    "        except NoSuchElementException:\n",
    "            stop = True\n",
    "        \n",
    "print(len(posting_urls), posting_urls[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "20 postings found\n",
      "2 postings found\n",
      "202 ['https://www.speakev.com/threads/peugeot-satnav-questions.177812/post-3441711', 'https://www.speakev.com/threads/peugeot-e208-wait-times.166986/post-3186684', 'https://www.speakev.com/threads/peugeot-e-208-charging-issue.165019/post-3143967', 'https://www.speakev.com/threads/peugeot-e-208-and-wallbox-wallbox-com.169166/post-3236693', 'https://www.speakev.com/threads/peugeot-e-traveller-electric-van.151120/post-2836498']\n"
     ]
    }
   ],
   "source": [
    "#### Store the urls of each post in the result page (When following Approach 2)\n",
    "\n",
    "# identify and store urls into the list\n",
    "stop = False\n",
    "while not stop:\n",
    "    postings = driver.find_elements(By.XPATH,'//*[@qid=\"search-results-title\"]')\n",
    "    print(str(len(postings)) + \" postings found\")\n",
    "    for posting in postings:\n",
    "        posting_urls.append(posting.get_attribute('href'))\n",
    "    # use try-except to avoid cases where the there is no \"next page\" button\n",
    "    try:\n",
    "        next_arrows = driver.find_elements(By.XPATH,'//a[@class=\"pageNavSimple-el pageNavSimple-el--next\"]')\n",
    "        if len(next_arrows)>0:\n",
    "            next_arrow = next_arrows[0]\n",
    "            if next_arrow.is_enabled():\n",
    "                # button is not interactable here, jump to the next page via the url directly\n",
    "                url_next_page = next_arrow.get_attribute('href')\n",
    "                driver.get(url_next_page)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                stop = True\n",
    "        else:\n",
    "            stop = True\n",
    "    except NoSuchElementException:\n",
    "        stop = True\n",
    "        \n",
    "print(len(posting_urls), posting_urls[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Store urls of posts and comments seperately\n",
    "### Urls of posts and comments are stored seperately, and each comment would be assigned with a feature value called 'post_id' to identify its parent post.\n",
    "### The reason to do so is that for posts that have more than 1 page of comments, beautiful soup can't scrape all data since the only one link is to the 1st page of each post, and it can't jump to the next page automatically. As a result, each page of a post (aka 'comment url') was iterated and recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 ['https://www.speakev.com/threads/fiat-doesnt-like-transparency.176618/', 'https://www.speakev.com/threads/fiat-600e-500x-replacement-to-be-announced-in-a-few-weeks.177628/', 'https://www.speakev.com/threads/new-fiat-panda-coming-most-affordable-ev.165895/', 'https://www.speakev.com/threads/new-fiat-panda-coming-most-affordable-ev.165895/page-2', 'https://www.speakev.com/threads/new-fiat-panda-coming-most-affordable-ev.165895/page-3']\n"
     ]
    }
   ],
   "source": [
    "#### The urls of posts are already stored in 'posting_urls'\n",
    "# The urls of comments would be scraped next\n",
    "# build up a list of urls of each post\n",
    "comment_urls = []\n",
    "for post in posting_urls:\n",
    "    # enter each post:\n",
    "    driver.get(post)\n",
    "    time.sleep(3)\n",
    "    # store urls of each page of a post into the list\n",
    "    # store the 1st page of each post first\n",
    "    comment_urls.append(post)\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        # use try-except to avoid cases where the there is no \"next page\" button\n",
    "        try:\n",
    "            next_arrows = driver.find_elements(By.XPATH,'//a[@class=\"pageNav-jump pageNav-jump--next california-page-nav-jump-next\"]')\n",
    "            if len(next_arrows)>0:\n",
    "                next_arrow = next_arrows[0]\n",
    "                if next_arrow.is_enabled():\n",
    "                    # button is not interactable here, jump to the next page via the url directly\n",
    "                    url_next_page = next_arrow.get_attribute('href')\n",
    "                    comment_urls.append(url_next_page)\n",
    "                    driver.get(url_next_page)\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    stop = True\n",
    "            else:\n",
    "                stop = True\n",
    "        except NoSuchElementException:\n",
    "            stop = True\n",
    "        \n",
    "print(len(comment_urls), comment_urls[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Store posts' and comments' urls into csv file\n",
    "# store posts urls\n",
    "df_post_url = pd.DataFrame({\"post_url\":posting_urls})\n",
    "df_post_url.to_csv(\"Fiat_post_url.csv\", sep=\",\", index=False, header=True)\n",
    "# store comments urls\n",
    "df_comment_url = pd.DataFrame({\"comment_url\":comment_urls})\n",
    "df_comment_url.to_csv(\"Fiat_comment_url.csv\", sep=\",\", index=False, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Scrape each post's information and information of comments under each post\n",
    "### (Store posts information and comments seperately)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create lists to store posts & comments information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create lists of post information\n",
    "l_post_title = []\n",
    "l_post_author = []\n",
    "l_post_date = []\n",
    "l_post_content = []\n",
    "l_comment_num = []\n",
    "l_post_like = []\n",
    "l_view = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create lists of comment information\n",
    "l_comment_id = []\n",
    "l_comment_post_id = []\n",
    "l_comment_author = []\n",
    "l_comment_date = []\n",
    "l_comment_content = []\n",
    "l_comment_like = []\n",
    "l_reply_id = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load urls for posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 92\n",
      "                                             post_url\n",
      "0   https://www.speakev.com/threads/fiat-doesnt-li...\n",
      "1   https://www.speakev.com/threads/fiat-600e-500x...\n",
      "2   https://www.speakev.com/threads/new-fiat-panda...\n",
      "3   https://www.speakev.com/threads/fiat-all-elect...\n",
      "4   https://www.speakev.com/threads/should-this-fo...\n",
      "..                                                ...\n",
      "76  https://www.speakev.com/threads/fiat-500e-conv...\n",
      "77  https://www.speakev.com/threads/500e-test-driv...\n",
      "78  https://www.speakev.com/threads/thinking-about...\n",
      "79  https://www.speakev.com/threads/fiat-500e-2020...\n",
      "80  https://www.speakev.com/threads/2021-fiat-500e...\n",
      "\n",
      "[81 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#### Load urls that were stored as .csv files\n",
    "post_url = pd.read_csv(\"Fiat_post_url.csv\")\n",
    "comment_url = pd.read_csv(\"Fiat_comment_url.csv\")\n",
    "print(len(post_url), len(comment_url))\n",
    "print(post_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Scrape posts' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoc\\AppData\\Local\\Temp\\ipykernel_8804\\2491627038.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(page.content, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "#### Scrape post information via each post url\n",
    "# Use for loop to get lists of required information\n",
    "for i in post_url[\"post_url\"]:\n",
    "    requests.adapters.DEFAULT_RETRIES = 5\n",
    "    try:\n",
    "        page = requests.get(i, headers=headers)\n",
    "    except:\n",
    "        page = requests.get(i, headers=headers)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# get the post information block\n",
    "    thread = soup.find(\"div\", class_ = \"MessageCard__container js-originalPostContainer\")\n",
    "\n",
    "# get the titile\n",
    "    try:\n",
    "        title = thread.find(\"h1\", class_ = \"MessageCard__thread-title\").text\n",
    "        l_post_title.append(title)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# get the author id\n",
    "    author = thread.find(\"div\", class_ = \"avatar-badge-wrapper\").find(\"a\").get(\"data-user-id\")\n",
    "    l_post_author.append(author)\n",
    "\n",
    "# get the post date\n",
    "    date = thread.find(\"a\", class_ = \"MessageCard__date-created\").find(\"time\").get(\"data-date-string\")\n",
    "    l_post_date.append(date)\n",
    "    \n",
    "# get the post content\n",
    "    content = thread.find(\"div\", class_ = \"bbWrapper\").text\n",
    "    l_post_content.append(content)\n",
    "\n",
    "# get the number of comments under each post\n",
    "# use try-except in case a post doesn't have any comments\n",
    "    try:\n",
    "        comment_num = thread.find(\"i\", class_ = \"fa fa-comment\").next_sibling[1:-9]\n",
    "        l_comment_num.append(comment_num)\n",
    "    except AttributeError:\n",
    "        l_comment_num.append(\"0\")\n",
    "\n",
    "# get the post likes\n",
    "# use try-except in case a post doesn't have any likes\n",
    "    try:\n",
    "        like = thread.find(\"a\", class_ = \"reactionsBar-link\").text\n",
    "        l_post_like.append(like)\n",
    "    except AttributeError:\n",
    "        l_post_like.append(\"0\")\n",
    "\n",
    "# get the post views\n",
    "# use try-except in case a post doesn't have any views\n",
    "    try:\n",
    "        view = thread.find(\"i\", class_ = \"fa fa-eye\").next_sibling[1:-7]\n",
    "        l_view.append(view)\n",
    "    except AttributeError:\n",
    "        l_view.append(\"0\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Store the post information to a dataframe and csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Store post information\n",
    "# Store data into the dataframe\n",
    "df_post = pd.DataFrame({\"Post_Title\":l_post_title, \"Author\":l_post_author, \"Date\":l_post_date,\n",
    "                        \"Post_Content\":l_post_content, \"Comment_Number\":l_comment_num,\n",
    "                        \"Net_Likes\":l_post_like, \"Views\":l_view})\n",
    "# Store data into the csv file\n",
    "df_post.to_csv(\"Fiat_post_info.csv\", sep=\",\", index=True, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Scrape comments' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post_Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Post_Content</th>\n",
       "      <th>Comment_Number</th>\n",
       "      <th>Net_Likes</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fiat doesn't like transparency</td>\n",
       "      <td>112918</td>\n",
       "      <td>Apr 4, 2023</td>\n",
       "      <td>I haven't seen my 500e yet although it stands ...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fiat 600e (500X replacement) to be announced i...</td>\n",
       "      <td>118330</td>\n",
       "      <td>May 21, 2023</td>\n",
       "      <td>The first spy shots are in, launch expected on...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FIAT 500e Steering Not as Light as Expected</td>\n",
       "      <td>74132</td>\n",
       "      <td>Feb 11, 2023</td>\n",
       "      <td>Had a new 500e for two weeks now and was expec...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 months so far for replacement body panels an...</td>\n",
       "      <td>79216</td>\n",
       "      <td>Apr 16, 2023</td>\n",
       "      <td>Our Fiat 500e (red) sustained front body damag...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Boot space</td>\n",
       "      <td>101780</td>\n",
       "      <td>May 19, 2023</td>\n",
       "      <td>Odd question, but this feels like the best pla...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Post_Title  Author  \\\n",
       "0           0                     Fiat doesn't like transparency  112918   \n",
       "1           1  Fiat 600e (500X replacement) to be announced i...  118330   \n",
       "2           2        FIAT 500e Steering Not as Light as Expected   74132   \n",
       "3           3  4 months so far for replacement body panels an...   79216   \n",
       "4           4                                         Boot space  101780   \n",
       "\n",
       "           Date                                       Post_Content  \\\n",
       "0   Apr 4, 2023  I haven't seen my 500e yet although it stands ...   \n",
       "1  May 21, 2023  The first spy shots are in, launch expected on...   \n",
       "2  Feb 11, 2023  Had a new 500e for two weeks now and was expec...   \n",
       "3  Apr 16, 2023  Our Fiat 500e (red) sustained front body damag...   \n",
       "4  May 19, 2023  Odd question, but this feels like the best pla...   \n",
       "\n",
       "   Comment_Number  Net_Likes  Views  \n",
       "0            17.0          1    888  \n",
       "1             5.0          0    603  \n",
       "2             9.0          0    944  \n",
       "3            10.0          0    691  \n",
       "4             5.0          0    341  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load the Fiat post information\n",
    "df_post = pd.read_csv(\"Fiat_post_info.csv\")\n",
    "df_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape comment information under each post via the comment url.\n",
    "#### Assign the post id to each comment\n",
    "# Use for loop to get lists of required information\n",
    "for i in comment_url[\"comment_url\"]:\n",
    "    requests.adapters.DEFAULT_RETRIES = 5\n",
    "    try:\n",
    "        page = requests.get(i, headers=headers)\n",
    "    except:\n",
    "        page = requests.get(i, headers=headers)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# get the post information block\n",
    "    thread = soup.find(\"div\", class_ = \"MessageCard__container js-originalPostContainer\")\n",
    "\n",
    "# get all the comment blocks within each page\n",
    "    comment = soup.select(\"[class*='MessageCard js-messageCard']\")\n",
    "    \n",
    "#### Get the post's unique id and the real id that is stored in the csv file\n",
    "# get the unique comment id of the post\n",
    "# (each post is also a comment on this website, so the comment id is taken as the unique id of each post)\n",
    "    try:\n",
    "        post_id = thread.select(\"[class*='MessageCard__content js-messageCard-content']\")[0].get(\"data-post-id\")\n",
    "    except AttributeError:\n",
    "        continue\n",
    "# get the post titile and assign the post id to the comment record\n",
    "    title = thread.find(\"h1\", class_ = \"MessageCard__thread-title\").text\n",
    "    try:\n",
    "        real_id = df_post.loc[df_post[\"Post_Title\"]==title, \"Unnamed: 0\"].values[0]\n",
    "    except IndexError:\n",
    "        real_id = -1\n",
    "    \n",
    "#### Use an embedded loop to scrape all comments' information within each page\n",
    "    for j in comment[1:]:\n",
    "\n",
    "    # get the unique id of each comment\n",
    "        comment_id = j.get(\"id\")[5:]\n",
    "        l_comment_id.append(comment_id)\n",
    "\n",
    "    # assign the same post id to comments under the same post\n",
    "        l_comment_post_id.append(real_id)\n",
    "\n",
    "    # get the author id\n",
    "        try:\n",
    "            author = j.find(\"div\", class_ = \"MessageCard__avatar\").find(\"a\").get(\"data-user-id\")\n",
    "            l_comment_author.append(author)\n",
    "        except AttributeError:\n",
    "            l_comment_author.append(\"None\")\n",
    "\n",
    "    # get the comment date\n",
    "        try:\n",
    "            date = j.find(\"a\", class_ = \"MessageCard__date-created\").find(\"time\").get(\"data-date-string\")\n",
    "            l_comment_date.append(date)\n",
    "        except AttributeError:\n",
    "            l_comment_date.append(\"None\")\n",
    "        \n",
    "    # get the comment content\n",
    "        try:\n",
    "            content = j.find(\"div\", class_ = \"bbWrapper\").text\n",
    "            l_comment_content.append(content)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    # get the comment likes\n",
    "    # use try-except in case a comment doesn't have any likes\n",
    "        try:\n",
    "            like = j.find(\"a\", class_ = \"reactionsBar-link\").text\n",
    "            l_comment_like.append(like)\n",
    "        except AttributeError:\n",
    "            l_comment_like.append(\"0\")\n",
    "\n",
    "    # get the replied comment's unique comment id if the comment is a reply to other comments\n",
    "    # use try-except in case a comment doesn't reply to any other comments\n",
    "    # set the reply id as \"None\" if the comment reply id is the same as the post's unique id \n",
    "    # (which is the self-contradictory foundamental setting of the website, where a comment can either reply the post or comment under the post without replying to it)\n",
    "        try:\n",
    "            try:\n",
    "                reply_id = j.find(\"blockquote\").find(\"a\").get(\"data-content-selector\")[6:]\n",
    "                if reply_id == post_id:\n",
    "                    l_reply_id.append(\"None\")\n",
    "                else:\n",
    "                    l_reply_id.append(reply_id)\n",
    "            except TypeError:\n",
    "                l_reply_id.append(\"None\")\n",
    "        except AttributeError:\n",
    "            l_reply_id.append(\"None\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Store the comment information to a dataframe and csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Store comment information\n",
    "# Store data into the dataframe\n",
    "df_comment = pd.DataFrame({\"Comment_id\":l_comment_id, \"Post_id\":l_comment_post_id, \"Author\":l_comment_author, \n",
    "                        \"Date\":l_comment_date, \"Comment_Content\":l_comment_content,\n",
    "                        \"Net_Likes\":l_comment_like, \"Reply_id\":l_reply_id})\n",
    "# Store data into the csv file\n",
    "df_comment.to_csv(\"Fiat_comment_info.csv\", sep=\",\", index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
